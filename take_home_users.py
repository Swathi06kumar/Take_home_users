# -*- coding: utf-8 -*-
"""Take home users

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/185-w2rNhmYTfLrIG1fHS0qHiGpJCyqkX
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import json
from pandas.io.json import json_normalize
from datetime import datetime, timedelta
import scipy.stats
import matplotlib.dates as mdates
import plotly.graph_objects as go
plt.style.use('bmh')

with open('/content/takehome_users.csv') as f:
    print(f)
with open('/content/takehome_user_engagement.csv') as f:
    print(f)

df = pd.read_csv('/content/takehome_users.csv', parse_dates = ['creation_time'], 
                    encoding = "cp1252")
df_eng = pd.read_csv('/content/takehome_user_engagement.csv',  parse_dates = ['time_stamp'], 
                       encoding = "cp1252")

df.info()

df.describe()

df_eng.info()

df_eng.describe()

df_eng.user_id.nunique()

df.head()

df_eng.head()

df['last_session_creation_time'] = pd.to_datetime(df['last_session_creation_time'] ,unit='s')

df.last_session_creation_time.dtypes

df['last_session_creation_time'].min(), df['last_session_creation_time'].max()

df['creation_time'].min(), df['creation_time'].max()

df1 = df_eng.copy()

df1['date'] = pd.to_datetime(df1.time_stamp.dt.date)

def rolling_count(df_group, frequency):
    return df_group.rolling(frequency, on='date')['user_id'].count()

df1['visits_7_days'] = df1.groupby('user_id', as_index=False, group_keys=False).apply(rolling_count, '7D')

df1.describe().T

df1[df1.visits_7_days >= 3.0]

df_adopted = df1.groupby('user_id')['visits_7_days'].max().reset_index()

df_adopted['adopted_user'] = df_adopted['visits_7_days'].apply(lambda x: 1 if x>=3 else 0)

df_adopted.head()

df_adopted.adopted_user.value_counts()

df_adopted.drop('visits_7_days', axis = 1, inplace = True)
df_adopted.rename(columns={"user_id": "object_id"}, inplace=True)

df_adopted.set_index("object_id", inplace = True)

df1_users = df.join(df_adopted, on = 'object_id', how='left')

df1_users.head()

df1_users.info()

df1_users['last_session_creation_time'].fillna(0, inplace = True)
df1_users['adopted_user'].fillna(0, inplace = True)

df1_users.describe().T

df1_users['email_domain'] = df1_users.email.apply(lambda x: x.split('@')[1])

df1_users['email_domain'].value_counts()

df1_users.drop(['object_id', 'name', 'email', 'email_domain'], axis = 1, inplace = True)

df1_users.invited_by_user_id.fillna(0, inplace=True)

df1_users['days_since_creation'] = (user_eng.time_stamp.max() - df1_users.creation_time).dt.days

df1_users.drop(['creation_time', 'last_session_creation_time'], axis = 1, inplace = True)

df1_users = pd.get_dummies(df1_users, columns=['creation_source'])

df1_users.describe().T

df1_users.corr()['adopted_user']

for col in ['opted_in_to_mailing_list', 'enabled_for_marketing_drip', 'creation_source_GUEST_INVITE',
           'creation_source_ORG_INVITE', 'creation_source_PERSONAL_PROJECTS', 'creation_source_SIGNUP',
           'creation_source_SIGNUP_GOOGLE_AUTH']:
    g = sns.FacetGrid(df1_users, hue = "adopted_user", height=3, aspect=1.5,)
    g.map(plt.hist, col, alpha=.5, bins = 20)
    g.add_legend()

g = sns.FacetGrid(df1_users, hue = "adopted_user", height=4, aspect=2,)
_ = g.map(plt.hist, 'days_since_creation', alpha=0.8, bins = 20)
_ = g.add_legend()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
import time
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

def cv_optimize(model, parameters, Xtrain, ytrain, n_folds = 5):
    """
    Cross validation. Function to hypertune the model "model" with the input paramete distribution using
    "parameters" on the training data.
    The output will be the best estimator whose average score on all folds will be best. 
    """
    clf = GridSearchCV(model, param_grid = parameters, cv = n_folds, scoring = 'accuracy')
    t0 = time.time()
    clf.fit(Xtrain, ytrain)
    time_fit = time.time() - t0 
    print('\n\n\n=============================',type(model).__name__,'=================================\n')
    print("It takes %.3f seconds for tuning " % (time_fit))
    print("BEST PARAMS", clf.best_params_)
    best = clf.best_estimator_
    return best
    
def do_classify(model, parameters, df, targetname, scale = True, cols_to_transform = 'numeric', 
                featurenames = 'all', train_size = 0.8):
      
    # Creating the X and y variables for our model
    if featurenames == 'all':
        X = df.drop([targetname], axis = 1)
    else:
        X = df[featurenames]
        
    y = df[targetname]
    
    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size = train_size)

    model = cv_optimize(model, parameters, Xtrain, ytrain)
    t0 = time.time()
    model = model.fit(Xtrain, ytrain)
    time_fit = time.time() - t0 
    print("It takes %.3f seconds for fitting" % (time_fit))
    training_accuracy = model.score(Xtrain, ytrain)
    test_accuracy = model.score(Xtest, ytest)
    precision = precision_score(ytest, model.predict(Xtest))
    recall = recall_score(ytest, model.predict(Xtest))
    AUC = roc_auc_score(ytest, model.predict_proba(Xtest)[:,1])
            
    print("Accuracy on training data: {:0.2f}".format(training_accuracy))
    print("Accuracy on test data:     {:0.2f}".format(test_accuracy))
    print("Precision on test data:    {:0.2f}".format(precision))
    print("Recall on test data:       {:0.2f}".format(recall))
    print("AUC on test data:          {:0.2f}".format(AUC))
    print("=======Confusion Matrix=========")
    print(confusion_matrix(ytest, model.predict(Xtest)))
    print("=======Classification report=======")
    print(classification_report(ytest, model.predict(Xtest)))
    print("="*100)
    print("="*100)
    print("="*100)
    return model, Xtrain, ytrain, Xtest, ytest

# Random Forest model
model_rf = RandomForestClassifier(class_weight='balanced') # adding balanced to handle the unbalanced data
parameters_rf = {
                 'n_estimators': [10, 25, 50, 75, 100],
                 'criterion': ["gini", "entropy"],
                 'max_depth': [3, 6, 10, 12],
                 'max_features': ['auto', 'sqrt']
                }

model_rf, Xtrain, ytrain, Xtest, ytest = do_classify(model_rf, parameters_rf, 
                                                                  df1_users, targetname = 'adopted_user')

feat_imp = pd.DataFrame({'importance':model_rf.feature_importances_})    
feat_imp['feature'] = Xtrain.columns
feat_imp.sort_values(by='importance', ascending=False, inplace=True)
    
feat_imp.sort_values(by='importance', inplace=True)
feat_imp = feat_imp.set_index('feature', drop=True)
_ = feat_imp.plot.barh(title = 'Random Forest feature importance', figsize = (12,7))